{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Project 3: Deep Learning - CNN Training and Deployment\n",
        "\n",
        "## Overview\n",
        "This project demonstrates the complete deep learning pipeline from CNN training to production deployment. We'll train a Fashion-MNIST classifier and deploy it as a REST API.\n",
        "\n",
        "## Project Structure (90 minutes)\n",
        "\n",
        "### Phase 1: Kick-off and Setup (0-5 min)\n",
        "- Environment verification\n",
        "- Dataset exploration\n",
        "\n",
        "### Phase 2: Local CNN Training (5-35 min)\n",
        "- CNN architecture design\n",
        "- Training loop implementation\n",
        "- Model evaluation and serialization\n",
        "\n",
        "### Phase 3: API Development (35-55 min)\n",
        "- FastAPI service creation\n",
        "- Endpoint implementation\n",
        "- Local API testing\n",
        "\n",
        "### Phase 4: Containerization (55-75 min)\n",
        "- Docker container build\n",
        "- Container testing\n",
        "\n",
        "### Phase 5: Cloud Deployment (75-90 min)\n",
        "- Remote deployment\n",
        "- Production API testing\n",
        "\n",
        "Let's begin!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Phase 1: Environment Setup and Data Loading\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Fashion-MNIST class names\n",
        "CLASS_NAMES = [\n",
        "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
        "]\n",
        "\n",
        "print(\"Environment setup complete!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Number of classes: {len(CLASS_NAMES)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Fashion-MNIST Dataset\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load training and test datasets\n",
        "train_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Training batches: {len(train_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")\n",
        "\n",
        "# Display sample images\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
        "for i in range(10):\n",
        "    image, label = train_dataset[i]\n",
        "    axes[i//5, i%5].imshow(image.squeeze(), cmap='gray')\n",
        "    axes[i//5, i%5].set_title(f'{CLASS_NAMES[label]}')\n",
        "    axes[i//5, i%5].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Phase 2: CNN Architecture Design (5-35 min)\n",
        "\n",
        "We'll create a 6-layer CNN architecture optimized for Fashion-MNIST classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define CNN Architecture\n",
        "class FashionCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionCNN, self).__init__()\n",
        "        # First convolutional block\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        # Second convolutional block\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        # Third convolutional block\n",
        "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(512)\n",
        "        self.conv6 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(512)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(512 * 3 * 3, 1024)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc3 = nn.Linear(512, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # First block\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool1(x)\n",
        "        \n",
        "        # Second block\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        # Third block\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = F.relu(self.bn6(self.conv6(x)))\n",
        "        x = self.pool3(x)\n",
        "        \n",
        "        # Flatten and fully connected\n",
        "        x = x.view(-1, 512 * 3 * 3)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "model = FashionCNN().to(device)\n",
        "\n",
        "# Model summary\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Model architecture: 6-layer CNN with {len(list(model.modules()))} modules\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Setup\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(progress_bar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "        \n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({\n",
        "            'Loss': f'{loss.item():.4f}',\n",
        "            'Acc': f'{100.*correct/total:.2f}%'\n",
        "        })\n",
        "    \n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_accuracies.append(epoch_acc)\n",
        "    \n",
        "    scheduler.step()\n",
        "    \n",
        "    print(f'Epoch {epoch+1}: Loss = {epoch_loss:.4f}, Accuracy = {epoch_acc:.2f}%')\n",
        "\n",
        "print(\"Training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Evaluation\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "test_loss = 0\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, target)\n",
        "        test_loss += loss.item()\n",
        "        \n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_total += target.size(0)\n",
        "        test_correct += (predicted == target).sum().item()\n",
        "        \n",
        "        # Calculate per-class accuracy\n",
        "        c = (predicted == target).squeeze()\n",
        "        for i in range(target.size(0)):\n",
        "            label = target[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "test_accuracy = 100 * test_correct / test_total\n",
        "test_loss = test_loss / len(test_loader)\n",
        "\n",
        "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "print('\\nPer-class accuracy:')\n",
        "for i in range(10):\n",
        "    if class_total[i] > 0:\n",
        "        print(f'{CLASS_NAMES[i]}: {100 * class_correct[i] / class_total[i]:.1f}%')\n",
        "\n",
        "# Save the model\n",
        "os.makedirs('models', exist_ok=True)\n",
        "model_path = 'models/fashion_cnn_model.pt'\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f'\\nModel saved to {model_path}')\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses)\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies)\n",
        "plt.title('Training Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Phase 3: API Development (35-55 min)\n",
        "\n",
        "Now we'll create a FastAPI service for model inference. The `app.py` file contains the complete API implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the API locally\n",
        "# First, run the API server: uvicorn app:app --host 0.0.0.0 --port 8000\n",
        "\n",
        "import requests\n",
        "import json\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "def test_api_health():\n",
        "    \"\"\"Test the health endpoint\"\"\"\n",
        "    try:\n",
        "        response = requests.get('http://localhost:8000/health')\n",
        "        print(f\"Health check status: {response.status_code}\")\n",
        "        print(f\"Response: {response.json()}\")\n",
        "        return response.status_code == 200\n",
        "    except Exception as e:\n",
        "        print(f\"API not running: {e}\")\n",
        "        return False\n",
        "\n",
        "def test_api_prediction(image_path):\n",
        "    \"\"\"Test image prediction endpoint\"\"\"\n",
        "    try:\n",
        "        with open(image_path, 'rb') as f:\n",
        "            files = {'file': f}\n",
        "            response = requests.post('http://localhost:8000/predict_image', files=files)\n",
        "        \n",
        "        print(f\"Prediction status: {response.status_code}\")\n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            print(f\"Prediction: {result['prediction']}\")\n",
        "            print(f\"Confidence: {result['confidence']:.4f}\")\n",
        "            return result\n",
        "        else:\n",
        "            print(f\"Error: {response.text}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Prediction test failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# To test the API, run:\n",
        "# 1. Start the API server in terminal: uvicorn app:app --host 0.0.0.0 --port 8000\n",
        "# 2. Then uncomment and run the following lines:\n",
        "\n",
        "# print(\"Testing API...\")\n",
        "# if test_api_health():\n",
        "#     print(\"API is running successfully!\")\n",
        "#     # Test with a sample image\n",
        "#     test_api_prediction('sample_images/test_pattern_1.png')\n",
        "# else:\n",
        "#     print(\"API is not running. Start it with: uvicorn app:app --host 0.0.0.0 --port 8000\")\n",
        "\n",
        "print(\"API testing functions ready!\")\n",
        "print(\"To test: Start API server and uncomment the test code above.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Phase 4: Containerization (55-75 min)\n",
        "\n",
        "Build and test the Docker container for deployment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Docker Container Commands\n",
        "# Run these commands in your terminal:\n",
        "\n",
        "docker_commands = \"\"\"\n",
        "# Build the Docker image\n",
        "docker build -t fashion-cnn-api .\n",
        "\n",
        "# Run the container locally\n",
        "docker run -p 8000:8000 fashion-cnn-api\n",
        "\n",
        "# Or use docker-compose (recommended)\n",
        "docker-compose up --build\n",
        "\n",
        "# Test the containerized API\n",
        "curl http://localhost:8000/health\n",
        "\n",
        "# Stop the container\n",
        "docker-compose down\n",
        "\"\"\"\n",
        "\n",
        "print(\"Docker Commands for Container Build and Test:\")\n",
        "print(docker_commands)\n",
        "\n",
        "# Create sample images for testing\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "def create_test_images():\n",
        "    \"\"\"Create sample test images if they don't exist\"\"\"\n",
        "    if not os.path.exists('sample_images'):\n",
        "        print(\"Creating sample images...\")\n",
        "        try:\n",
        "            subprocess.run(['python', 'create_sample_images.py'], check=True)\n",
        "            print(\"Sample images created successfully!\")\n",
        "        except subprocess.CalledProcessError:\n",
        "            print(\"Failed to create sample images. Create them manually.\")\n",
        "    else:\n",
        "        print(\"Sample images directory already exists.\")\n",
        "\n",
        "def test_docker_container():\n",
        "    \"\"\"Test the Docker container endpoints\"\"\"\n",
        "    commands = [\n",
        "        \"# Test health endpoint\",\n",
        "        \"curl -X GET http://localhost:8000/health\",\n",
        "        \"\",\n",
        "        \"# Test prediction endpoint with sample image\",\n",
        "        \"curl -X POST http://localhost:8000/predict_image -F 'file=@sample_images/test_pattern_1.png'\",\n",
        "        \"\",\n",
        "        \"# View API documentation\",\n",
        "        \"curl -X GET http://localhost:8000/docs\"\n",
        "    ]\n",
        "    \n",
        "    print(\"Container Test Commands:\")\n",
        "    for cmd in commands:\n",
        "        print(cmd)\n",
        "\n",
        "create_test_images()\n",
        "test_docker_container()\n",
        "\n",
        "print(\"\\nContainer deployment instructions:\")\n",
        "print(\"1. Build: docker build -t fashion-cnn-api .\")\n",
        "print(\"2. Run: docker run -p 8000:8000 fashion-cnn-api\")\n",
        "print(\"3. Test: curl http://localhost:8000/health\")\n",
        "print(\"4. Stop: docker stop <container_id>\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Phase 5: Cloud Deployment (75-90 min)\n",
        "\n",
        "Deploy the containerized model to Hetzner VM and test the production API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cloud Deployment Commands\n",
        "# Replace YOUR_SERVER_IP with your actual Hetzner server IP\n",
        "\n",
        "deployment_commands = \"\"\"\n",
        "# 1. Copy files to server\n",
        "scp -r . username@YOUR_SERVER_IP:/home/username/fashion-cnn-api/\n",
        "\n",
        "# 2. SSH into server\n",
        "ssh username@YOUR_SERVER_IP\n",
        "\n",
        "# 3. Navigate to project directory\n",
        "cd /home/username/fashion-cnn-api/\n",
        "\n",
        "# 4. Build and run with docker-compose\n",
        "docker-compose up --build -d\n",
        "\n",
        "# 5. Check container status\n",
        "docker ps\n",
        "\n",
        "# 6. View logs\n",
        "docker logs fashion_cnn_classifier\n",
        "\n",
        "# 7. Test the deployed API\n",
        "curl http://YOUR_SERVER_IP:8000/health\n",
        "\"\"\"\n",
        "\n",
        "print(\"Cloud Deployment Commands:\")\n",
        "print(deployment_commands)\n",
        "\n",
        "# Production API Testing\n",
        "def test_production_api(server_ip):\n",
        "    \"\"\"Test the production API deployment\"\"\"\n",
        "    import requests\n",
        "    \n",
        "    base_url = f\"http://{server_ip}:8000\"\n",
        "    \n",
        "    try:\n",
        "        # Test health endpoint\n",
        "        response = requests.get(f\"{base_url}/health\")\n",
        "        print(f\"Health check: {response.status_code}\")\n",
        "        print(f\"Response: {response.json()}\")\n",
        "        \n",
        "        # Test prediction endpoint\n",
        "        with open('sample_images/test_pattern_1.png', 'rb') as f:\n",
        "            files = {'file': f}\n",
        "            response = requests.post(f\"{base_url}/predict_image\", files=files)\n",
        "        \n",
        "        print(f\"Prediction test: {response.status_code}\")\n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            print(f\"Prediction: {result['prediction']}\")\n",
        "            print(f\"Confidence: {result['confidence']:.4f}\")\n",
        "        \n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Production API test failed: {e}\")\n",
        "        return False\n",
        "\n",
        "# Monitoring and troubleshooting commands\n",
        "monitoring_commands = \"\"\"\n",
        "# Monitor container performance\n",
        "docker stats fashion_cnn_classifier\n",
        "\n",
        "# Check container logs\n",
        "docker logs -f fashion_cnn_classifier\n",
        "\n",
        "# Restart container if needed\n",
        "docker-compose restart\n",
        "\n",
        "# Update deployment\n",
        "docker-compose pull\n",
        "docker-compose up --build -d\n",
        "\n",
        "# Cleanup old images\n",
        "docker image prune -f\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nProduction Monitoring Commands:\")\n",
        "print(monitoring_commands)\n",
        "\n",
        "# Final summary\n",
        "print(\"\\n=== DEPLOYMENT COMPLETE ===\")\n",
        "print(\"Your Fashion-MNIST CNN classifier is now deployed!\")\n",
        "print(\"API Endpoints:\")\n",
        "print(\"- Health: http://YOUR_SERVER_IP:8000/health\")\n",
        "print(\"- Predict: http://YOUR_SERVER_IP:8000/predict_image\")\n",
        "print(\"- Docs: http://YOUR_SERVER_IP:8000/docs\")\n",
        "print(\"\\nTo test production API, update YOUR_SERVER_IP and run:\")\n",
        "print(\"test_production_api('YOUR_SERVER_IP')\")\n",
        "\n",
        "print(\"\\n🎉 Project Complete! 🎉\")\n",
        "print(\"You've successfully:\")\n",
        "print(\"✅ Trained a CNN model\")\n",
        "print(\"✅ Created a FastAPI service\")\n",
        "print(\"✅ Containerized the application\")\n",
        "print(\"✅ Deployed to cloud infrastructure\")\n",
        "print(\"✅ Tested the production API\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
