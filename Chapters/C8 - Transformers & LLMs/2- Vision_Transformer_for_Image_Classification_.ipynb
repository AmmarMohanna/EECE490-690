{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"environment":{"name":"tf2-gpu.2-4.m61","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"VaImTdn2Cz5K"},"source":["# Vision Transformer for Image Classification\n","----"]},{"cell_type":"markdown","source":["In this notebook, we will visualize how transformers use attention mechanism in a Computer Vision environment. The basic ViT architecture is used, however with only one transformer layer with one (or four) head(s) for simplicity.\n","\n","The model is trained on CIFAR-10 classification task."],"metadata":{"id":"8y3J92PjZx2t"}},{"cell_type":"markdown","source":["## Installing and Importing Libraries\n","\n","We need to install the libraries we are going to use:\n","\n","**numpy**: library used for numerical operations\n","\n","**tensorflow**: popular framework for deep learning\n","\n","**keras**: high-level API for building and training neural networks, which is now integrade to TensorFlow\n","\n","**tensorflow_addons**: repository of additional functionalities and custom layers for TensorFlow\n","\n","**matplotlib**: library to create visualizations\n","\n","**opencv(cv2)**: computer vision library useful for image processing. cv2_imshow allows us to display images directly on Colab\n","\n","**imageio**: library for reading and writing a wide variety of image data\n","\n","**PIL(Python Image Library)**: used for opening, manipulating and saving many different image formats"],"metadata":{"id":"MXGzBIarO2cK"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RarzyWsKEFy4","outputId":"862f3243-7b49-431a-be28-2c8d80e5523c","executionInfo":{"status":"ok","timestamp":1732821333326,"user_tz":-120,"elapsed":9606,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}}},"source":["!pip install -U tensorflow-addons"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow-addons\n","  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.2)\n","Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n","  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n","Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/611.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.7/611.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n","Installing collected packages: typeguard, tensorflow-addons\n","  Attempting uninstall: typeguard\n","    Found existing installation: typeguard 4.4.1\n","    Uninstalling typeguard-4.4.1:\n","      Successfully uninstalled typeguard-4.4.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","inflect 7.4.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n"]}]},{"cell_type":"code","metadata":{"id":"V5TTEF9BWmZK","outputId":"c879bf12-c3f4-4f26-83d7-9e221e8a6adb","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"error","timestamp":1732821343953,"user_tz":-120,"elapsed":10630,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}}},"source":["import numpy as np\n","np.random.seed(1337)\n","\n","import tensorflow as tf\n","tf.random.set_seed(1234)\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import tensorflow_addons as tfa\n","\n","import matplotlib.pyplot as plt\n","import cv2\n","from google.colab.patches import cv2_imshow\n","\n","import imageio\n","\n","import PIL\n","from PIL import Image, ImageDraw, ImageFont"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n","\n","TensorFlow Addons (TFA) has ended development and introduction of new features.\n","TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n","Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n","\n","For more information see: https://github.com/tensorflow/addons/issues/2807 \n","\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.13.0 and strictly below 2.16.0 (nightly versions are not supported). \n"," The versions of TensorFlow you are currently using is 2.17.1 and is not supported. \n","Some things might work, some things might not.\n","If you were to encounter a bug, do not file an issue.\n","If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n","You can find the compatibility matrix in TensorFlow Addon's readme:\n","https://github.com/tensorflow/addons\n","  warnings.warn(\n"]},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'keras.src.engine'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-54931e5111c8>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_addons/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Local project imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_addons/activations/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Additional activation functions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgelu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhardshrink\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhardshrink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlisht\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlisht\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_addons/activations/gelu.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorLike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/types.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# New versions of Keras require importing from `keras.src` when\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# importing internal symbols.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2.5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.src.engine'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","source":["## Defining the Video Function\n","We create a function that will take our generated images and create a video of them. We will later download this video to watch it on our own machine."],"metadata":{"id":"6FUzTssCSXGI"}},{"cell_type":"code","metadata":{"id":"dhnftiBVXUfp","executionInfo":{"status":"aborted","timestamp":1732821343954,"user_tz":-120,"elapsed":19,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}}},"source":["def make_video(movieName, img_list, att_list):\n","  with imageio.get_writer(movieName, mode='I') as writer:\n","      h,w = img_list[0].shape[0:2]\n","      for idx, img_i in enumerate(img_list):\n","        canvas = PIL.Image.new('RGBA', (w*2, h), 'black')\n","        canvas.paste(Image.fromarray(img_list[idx].astype(np.uint8)), (0, 0))\n","        feature_map = Image.fromarray(att_list[idx].astype(np.uint8))\n","        canvas.paste(feature_map, (w, 0))\n","        writer.append_data(np.array(canvas))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Loading the Dataset\n","We now load our dataset from the keras datasets and already split into training and testing splits. We will load the cifar100 dataset from Keras."],"metadata":{"id":"zn3PMZWGVUsN"}},{"cell_type":"code","metadata":{"id":"jCCffhUF0FJq","executionInfo":{"status":"aborted","timestamp":1732821343955,"user_tz":-120,"elapsed":18,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}}},"source":["num_classes = 10\n","input_shape = (32, 32, 3)\n","\n","(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n","\n","print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n","print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preparing the Model"],"metadata":{"id":"gchoUM9SYVhC"}},{"cell_type":"markdown","source":["### Setting our hyperparameters\n","First thing we will do is set our hyperparamaters to be used later in our model. Doing things modularly is considered good practice, and facilitates the creation of multiple models later on."],"metadata":{"id":"R5XatDGgYqok"}},{"cell_type":"code","metadata":{"id":"Wt-FoMgI2jU5","executionInfo":{"status":"aborted","timestamp":1732821343955,"user_tz":-120,"elapsed":17,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}}},"source":["learning_rate = 0.001\n","weight_decay = 0.0001\n","batch_size = 256\n","num_epochs = 15 # 100\n","image_size = 72  # We'll resize input images to this size\n","patch_size = 6  # Size of the patches to be extract from the input images\n","num_patches = (image_size // patch_size) ** 2\n","projection_dim = 64\n","num_heads = 4 #4\n","transformer_units = [\n","    projection_dim * 2,\n","    projection_dim,\n","]  # Size of the transformer layers\n","transformer_layers = 1 #8\n","mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Augmenting our data\n","We create now a pipeline for data augmentation, which is increasing the amount of data available artifically by making slight changes in it.\n","\n","However, in our case we will only normalize our data and resize it to a specific size defined earlier so that we can feed it to our model properly."],"metadata":{"id":"LTD6oXU_Y7S6"}},{"cell_type":"code","metadata":{"id":"Ncfq-VKf2jW5","executionInfo":{"status":"aborted","timestamp":1732821343955,"user_tz":-120,"elapsed":16,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}}},"source":["data_augmentation = keras.Sequential(\n","    [\n","        layers.experimental.preprocessing.Normalization(),\n","        layers.experimental.preprocessing.Resizing(image_size, image_size),\n","    ],\n","    name=\"data_augmentation\",\n",")\n","# Compute the mean and the variance of the training data for normalization.\n","data_augmentation.layers[0].adapt(x_train)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Creating the Multi-Layer Perceptron (MLP) Function"],"metadata":{"id":"heFXsSehaHpc"}},{"cell_type":"code","metadata":{"id":"m5DtGX4T2jaW","executionInfo":{"status":"aborted","timestamp":1732821343955,"user_tz":-120,"elapsed":16,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}}},"source":["def mlp(x, hidden_units, dropout_rate):\n","    for units in hidden_units:\n","        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n","        x = layers.Dropout(dropout_rate)(x)\n","    return x\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Creating the Patches Layer\n","We create a custom Keras Layer that will extract image patches from our input images and reshape them into a useful format."],"metadata":{"id":"pDYOyaFVadr9"}},{"cell_type":"code","metadata":{"id":"yAtnBW692jcL","executionInfo":{"status":"aborted","timestamp":1732821343955,"user_tz":-120,"elapsed":16,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}}},"source":["class Patches(layers.Layer):\n","    def __init__(self, patch_size):\n","        super(Patches, self).__init__()\n","        self.patch_size = patch_size\n","\n","    def call(self, images):\n","        batch_size = tf.shape(images)[0]\n","        patches = tf.image.extract_patches(\n","            images=images,\n","            sizes=[1, self.patch_size, self.patch_size, 1],\n","            strides=[1, self.patch_size, self.patch_size, 1],\n","            rates=[1, 1, 1, 1],\n","            padding=\"VALID\",\n","        )\n","        patch_dims = patches.shape[-1]\n","        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n","        return patches\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can now visualize the patches in our images, we will take one example"],"metadata":{"id":"H9qfaUv9a3Cs"}},{"cell_type":"code","metadata":{"id":"wi21ndVb2jd5","executionInfo":{"status":"aborted","timestamp":1732821343956,"user_tz":-120,"elapsed":16,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}}},"source":["plt.figure(figsize=(4, 4))\n","# image = x_train[np.random.choice(range(x_train.shape[0]))]\n","\n","image = x_train[77]\n","\n","plt.imshow(np.squeeze(image.astype(\"uint8\")), cmap='gray')\n","\n","plt.axis(\"off\")\n","\n","resized_image = tf.image.resize(\n","    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",")\n","patches = Patches(patch_size)(resized_image)\n","print(f\"Image size: {image_size} X {image_size} pixels\")\n","print(f\"Patch size: {patch_size} X {patch_size} pixels\")\n","print(f\"Patches per image: {patches.shape[1]} (image_size // patch_size) ** 2\")\n","print(f\"Elements per patch: {patches.shape[-1]} w*h*3\")\n","\n","n = int(np.sqrt(patches.shape[1]))\n","plt.figure(figsize=(4, 4))\n","for i, patch in enumerate(patches[0]):\n","    ax = plt.subplot(n, n, i + 1)\n","    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n","    plt.imshow(np.squeeze(patch_img.numpy().astype(\"uint8\")), cmap='gray')\n","    plt.axis(\"off\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Creating the Patch Encoder Layer"],"metadata":{"id":"G6BXU-bdbWWv"}},{"cell_type":"markdown","source":["We now create the PatchEncoder Keras Layer which is used to encode the Patches generated earlier and add positional information to them."],"metadata":{"id":"x5B26lijbCiF"}},{"cell_type":"code","metadata":{"id":"WlanBtQk2jfu","executionInfo":{"status":"aborted","timestamp":1732821343956,"user_tz":-120,"elapsed":16,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}}},"source":["class PatchEncoder(layers.Layer):\n","    def __init__(self, num_patches, projection_dim):\n","        super(PatchEncoder, self).__init__()\n","        self.num_patches = num_patches\n","        self.projection = layers.Dense(units=projection_dim)\n","        self.position_embedding = layers.Embedding(input_dim=num_patches, output_dim=projection_dim)\n","\n","    def call(self, patch):\n","        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n","        encoded = self.projection(patch) + self.position_embedding(positions)\n","        return encoded"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Creating the Model\n","With everything prepared, we can create our model and add our custom layers to it"],"metadata":{"id":"bGopFABbbeqX"}},{"cell_type":"code","metadata":{"id":"QBriHQvO2jiz","executionInfo":{"status":"aborted","timestamp":1732821343957,"user_tz":-120,"elapsed":21042,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}}},"source":["def create_vit_classifier():\n","    inputs = layers.Input(shape=input_shape)\n","    # Augment data.\n","    augmented = data_augmentation(inputs)\n","    # Create patches.\n","    patches = Patches(patch_size)(augmented)\n","    # Encode patches.\n","    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n","\n","    # no pos encoding!\n","    # encoded_patches = PatchEncoder2(num_patches, projection_dim)(patches)\n","\n","    # Create multiple layers of the Transformer block.\n","    # attention_scores_list = []\n","    for _ in range(transformer_layers):\n","        # Layer normalization 1.\n","        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n","        # Create a multi-head attention layer.\n","        attention_output, attention_scores_items = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=projection_dim, dropout=0.1)(x1, x1, return_attention_scores=True)\n","        # attention_scores_list.append(attention_scores_items)\n","        # Skip connection 1.\n","        x2 = layers.Add()([attention_output, encoded_patches])\n","        # Layer normalization 2.\n","        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n","        # MLP.\n","        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n","        # Skip connection 2.\n","        encoded_patches = layers.Add()([x3, x2])\n","\n","    # Create a [batch_size, projection_dim] tensor.\n","    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n","    representation = layers.Flatten()(representation)\n","    representation = layers.Dropout(0.5)(representation)\n","    # Add MLP.\n","    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n","    # Classify outputs.\n","    logits = layers.Dense(num_classes)(features)\n","    # Create the Keras model.\n","    model = keras.Model(inputs=inputs, outputs=logits)\n","\n","    model_att = keras.Model(inputs=inputs, outputs=attention_scores_items)\n","    return model, model_att"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training and Evaluating the Model\n","We create now a function that will Train and Evaluate our model and save the best weights for later use.\n","\n","We will pass the necessary parameters that we set earlier for the optimizer.\n","\n","For the metrics, we will use Categorical Accuracy and TopK Categorical Accuracy = 5."],"metadata":{"id":"Dr3y4vmmbziR"}},{"cell_type":"code","metadata":{"id":"v6n1K21R2jk8","executionInfo":{"status":"aborted","timestamp":1732821343957,"user_tz":-120,"elapsed":21040,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}}},"source":["def run_experiment(model):\n","    optimizer = tfa.optimizers.AdamW(\n","        learning_rate=learning_rate, weight_decay=weight_decay\n","    )\n","\n","    model.compile(\n","        optimizer=optimizer,\n","        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","        metrics=[\n","            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n","            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n","        ],\n","    )\n","\n","    checkpoint_filepath = \"/tmp/checkpoint\"\n","    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n","        checkpoint_filepath,\n","        monitor=\"val_accuracy\",\n","        save_best_only=True,\n","        save_weights_only=True,\n","    )\n","\n","    history = model.fit(\n","        x=x_train,\n","        y=y_train,\n","        batch_size=batch_size,\n","        epochs=num_epochs,\n","        validation_split=0.1,\n","        callbacks=[checkpoint_callback],\n","    )\n","\n","    model.load_weights(checkpoint_filepath)\n","    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n","    print(\"-\"*50)\n","    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n","    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n","\n","    return history\n","\n","# We save it as a variable to run the function appropriately later on\n","vit_classifier, at_model = create_vit_classifier()\n","vit_classifier.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cT5W9D-K2jnO","executionInfo":{"status":"aborted","timestamp":1732821343957,"user_tz":-120,"elapsed":21039,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}}},"source":["at_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will now use the run_experiment function the variable vit_classifier that we created earlier"],"metadata":{"id":"f2InAu5WkwlP"}},{"cell_type":"code","metadata":{"id":"-4lg4EWz2jpX","executionInfo":{"status":"aborted","timestamp":1732821343958,"user_tz":-120,"elapsed":21039,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}}},"source":["history = run_experiment(vit_classifier)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualizing our Results\n","Let's start by plotting some examples from our dataset"],"metadata":{"id":"5DiZrHtIk4-J"}},{"cell_type":"code","metadata":{"id":"e4CqTtSGrj5w","executionInfo":{"status":"aborted","timestamp":1732821343958,"user_tz":-120,"elapsed":21038,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}}},"source":["plt.figure(figsize=(14, 14))\n","for i in range(100):\n","    ax = plt.subplot(10, 10, i + 1)\n","    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n","    plt.imshow(np.squeeze(x_train[i]))\n","    plt.title(str(i))\n","    plt.axis(\"off\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let's select one example that we will base our remaining experiments upon"],"metadata":{"id":"K4tUKyiTlMM3"}},{"cell_type":"code","metadata":{"id":"zm66hd1B2jsQ","executionInfo":{"status":"aborted","timestamp":1732821343958,"user_tz":-120,"elapsed":21036,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}}},"source":["selected_image_index = 91\n","plt.figure(figsize=(5, 5))\n","image = x_train[selected_image_index]\n","plt.imshow(np.squeeze(image))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we create a set of attention maps to display each self-attention head's scores in the multi-head attention we created"],"metadata":{"id":"YGMotg-x5Fbp"}},{"cell_type":"code","metadata":{"id":"zFiMEXLImjYt","executionInfo":{"status":"aborted","timestamp":1732821343959,"user_tz":-120,"elapsed":21036,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}}},"source":["plt.figure(figsize=(9, 7))\n","for h in range(4):\n","    ax = plt.subplot(2, 2, h + 1)\n","    att_results = at_model(np.expand_dims(x_train[selected_image_index], axis=0))\n","    plt.imshow(att_results[0,h,:])\n","    plt.title(str(h))\n","    plt.colorbar()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we will visualize the attention map for each attention head, we will create a video based on the generated images"],"metadata":{"id":"2_pQfR0_6qTr"}},{"cell_type":"code","metadata":{"id":"EGjbHEUe8V5Q","executionInfo":{"status":"aborted","timestamp":1732821343959,"user_tz":-120,"elapsed":21035,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}}},"source":["head_index =  2\n","att_list = []\n","patch_list = []\n","std_list = []\n","n = int(np.sqrt(patches.shape[1]))\n","plt.figure(figsize=(15, 15))\n","for selected_q_index in range(n*n):\n","  ax = plt.subplot(n, n, selected_q_index + 1)\n","\n","  att_mask  = tf.reshape(att_results[0, head_index, selected_q_index, :], [n, n])\n","  feature_map = att_mask.numpy()\n","  std_list.append(np.std(feature_map))\n","  feature_map /= np.max(feature_map)\n","  feature_map *= 255\n","  feature_map = np.clip(feature_map, 0, 255).astype('uint8')\n","  feature_map = cv2.resize(feature_map, (image_size, image_size))\n","\n","\n","  feature_map = cv2.applyColorMap(feature_map, cv2.COLORMAP_JET)\n","  feature_map = cv2.cvtColor(feature_map, cv2.COLOR_BGR2RGB)\n","\n","  image_patch = cv2.resize(image, (image_size, image_size))\n","  n = int(np.sqrt(num_patches))\n","  x = selected_q_index % n\n","  y = selected_q_index // n\n","  cv2.rectangle(feature_map, (x*patch_size, y*patch_size), ((x*patch_size)+patch_size, (y*patch_size)+patch_size), 255, 2)\n","  cv2.rectangle(image_patch, (x*patch_size, y*patch_size), ((x*patch_size)+patch_size, (y*patch_size)+patch_size), 255, 2)\n","\n","  # feature_map = feature_map + (0.3 * image_patch).astype('uint8')\n","  # image_patch = image_patch + (0.3 * feature_map).astype('uint8')\n","  image_patch = cv2.addWeighted(image_patch,1.0,feature_map,0.2,0)\n","  att_list.append(feature_map)\n","  patch_list.append(image_patch)\n","  # cv2_imshow(feature_map)\n","  plt.imshow(feature_map)\n","  # plt.imshow(image_patch)\n","  #plt.imshow(image_patch, cmap='gray')\n","  # cv2_imshow(feature_map)\n","  plt.axis(\"off\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Creating our Video\n","We now create a video from the heatmap images above. We will do that by using the make_video function"],"metadata":{"id":"5ll7tQTo7oj1"}},{"cell_type":"code","metadata":{"id":"-fRRcMgH8V8J","executionInfo":{"status":"aborted","timestamp":1732821343959,"user_tz":-120,"elapsed":21034,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}}},"source":["make_video('att_cifar_L1H1_91_h3.mp4', patch_list, att_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We get a warning saying that the input image is not in the correct dimensions. Now we will resize each patch image to the specified size."],"metadata":{"id":"YUlXyQUC7yq9"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","\n","# Load the patch_list variable (assuming it's a list of image patches)\n","# patch_list should contain a list of numpy arrays, each representing an image patch\n","\n","# Define a function to resize an individual patch to the nearest multiple of 16\n","def resize_patch(patch):\n","    new_width = ((patch.shape[1] + 15) // 16) * 16\n","    new_height = ((patch.shape[0] + 15) // 16) * 16\n","    resized_patch = cv2.resize(patch, (new_width, new_height))\n","    return resized_patch\n","\n","# Resize each patch in the patch_list\n","resized_patch_list = [resize_patch(patch) for patch in patch_list]\n","\n","# Now, resized_patch_list contains the resized patches, and you can proceed with your video creation process\n"],"metadata":{"id":"6cHLzclnXVMT","executionInfo":{"status":"aborted","timestamp":1732821344369,"user_tz":-120,"elapsed":2,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's run the make_video function again"],"metadata":{"id":"NbXb5nDm8OpK"}},{"cell_type":"code","source":["make_video('att_cifar_L1H1_91_h3.mp4', resized_patch_list, att_list)"],"metadata":{"id":"riJQS5oBXcJZ","executionInfo":{"status":"aborted","timestamp":1732821344369,"user_tz":-120,"elapsed":1,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Download the video locally and enjoy!"],"metadata":{"id":"fsD69aoTY47u"}}]}