{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Representing Images as Matrices**\n","\n","### **Introduction and Objectives**\n","\n","In this lab, we will explore how images are represented numerically in a machine learning pipeline. By the end of this lab, you will:\n","- Understand how to load images as NumPy arrays.\n","- Learn how color channels (RGB) translate to 3D arrays.\n","- Perform simple manipulations (e.g., resizing, reshaping, normalizing).\n","- Prepare images as model inputs in a way that is acceptable to typical ML frameworks.\n"],"metadata":{"id":"aUhEscQvtBPC"}},{"cell_type":"markdown","source":["[![Watch the video](https://img.youtube.com/vi/06OHflWNCOE/0.jpg)](https://www.youtube.com/watch?v=06OHflWNCOE)\n"],"metadata":{"id":"Q2Mp5a2w1bSP"}},{"cell_type":"markdown","source":["## üìö **Imports and Libraries**\n","\n","**Why these libraries?**\n","- **OpenCV (cv2)**: A powerful library for reading, writing, and manipulating images and videos. We will use its functions to load images, resize them, convert them to grayscale, and apply a threshold.\n","\n","### üîó **References:**\n","- [OpenCV Documentation](https://docs.opencv.org/master/)\n"],"metadata":{"id":"5kh_RsNAvDl8"}},{"cell_type":"markdown","source":["## **1Ô∏è‚É£ Load Grayscale and Apply Threshold**"],"metadata":{"id":"DU8jC2JdhYPj"}},{"cell_type":"code","source":["!git clone https://github.com/AmmarMohanna/EECE490-690.git"],"metadata":{"id":"7Dwfw887vhoo","executionInfo":{"status":"ok","timestamp":1756319955984,"user_tz":-180,"elapsed":3053,"user":{"displayName":"Mohamad Zbib","userId":"15661866840379818764"}},"outputId":"b418e39c-aa12-4fa2-8c13-c48088e95265","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'EECE490-690'...\n","remote: Enumerating objects: 212, done.\u001b[K\n","remote: Counting objects: 100% (212/212), done.\u001b[K\n","remote: Compressing objects: 100% (181/181), done.\u001b[K\n","remote: Total 212 (delta 51), reused 179 (delta 24), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (212/212), 36.34 MiB | 22.81 MiB/s, done.\n","Resolving deltas: 100% (51/51), done.\n"]}]},{"cell_type":"code","source":["import cv2\n","\n","# We will demonstrate loading an image in grayscale mode, resizing it, and applying a threshold.\n","image = cv2.imread('/content/EECE490-690/Chapters/C2 - Preparing Data for Statistical Machine Learning Algorithms /dino.png', cv2.IMREAD_GRAYSCALE)  # Load as grayscale\n","image = cv2.resize(image, (15, 15))  # Resize to 15x15 for simplicity\n","\n","# Apply a binary threshold\n","# Any pixel value above 100 becomes 1, below 100 becomes 0.\n","_, binary_image = cv2.threshold(image, 100, 1, cv2.THRESH_BINARY)\n","\n","# Display the resulting binary image array\n","binary_image\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"id":"d1pBfH3w7DXe","executionInfo":{"status":"ok","timestamp":1756319956336,"user_tz":-180,"elapsed":345,"user":{"displayName":"Mohamad Zbib","userId":"15661866840379818764"}},"outputId":"d36d6178-65d4-48a8-bd4a-49a8547873ca"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=uint8)"],"text/html":["<style>\n","      .ndarray_repr .ndarray_raw_data {\n","        display: none;\n","      }\n","      .ndarray_repr.show_array .ndarray_raw_data {\n","        display: block;\n","      }\n","      .ndarray_repr.show_array .ndarray_image_preview {\n","        display: none;\n","      }\n","      </style>\n","      <div id=\"id-7401c71c-ee2e-4db1-b206-2a702d52f810\" class=\"ndarray_repr\"><pre>ndarray (15, 15) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAAPCAAAAAAevcqWAAAAIklEQVR4nGNkZEABTAxE8P+j8RnR+Bj6/8O1MJJjHxXdAwAG2gQdTTA/FAAAAABJRU5ErkJggg==\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=uint8)</pre></div><script>\n","      (() => {\n","      const titles = ['show data', 'hide data'];\n","      let index = 0\n","      document.querySelector('#id-7401c71c-ee2e-4db1-b206-2a702d52f810 button').onclick = (e) => {\n","        document.querySelector('#id-7401c71c-ee2e-4db1-b206-2a702d52f810').classList.toggle('show_array');\n","        index = (++index) % 2;\n","        document.querySelector('#id-7401c71c-ee2e-4db1-b206-2a702d52f810 button').textContent = titles[index];\n","        e.preventDefault();\n","        e.stopPropagation();\n","      }\n","      })();\n","    </script>"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["**Explanation:**\n","\n","1. **`cv2.imread('/content/dino.png', cv2.IMREAD_GRAYSCALE)`** loads the specified image file in grayscale mode.  \n","2. **`cv2.resize(image, (15, 15))`** resizes the image to a 15√ó15 pixel dimension.  \n","3. **`cv2.threshold(image, 100, 1, cv2.THRESH_BINARY)`** applies a binary threshold:  \n","   - Threshold value = 100  \n","   - Values above 100 become 1  \n","   - Values below 100 become 0  \n","\n","The output, `binary_image`, is a 2D array (matrix) of 0s and 1s. Each cell in the matrix corresponds to a pixel in the thresholded image.\n"],"metadata":{"id":"AR5NAGeexltX"}},{"cell_type":"markdown","source":["## **2Ô∏è‚É£ Load and Display Grayscale**"],"metadata":{"id":"HySx5pKSh3nb"}},{"cell_type":"code","source":["# Load and display the grayscale image as a matrix\n","image = cv2.imread('/content/EECE490-690/Chapters/C2 - Preparing Data for Statistical Machine Learning Algorithms /dino.png', cv2.IMREAD_GRAYSCALE)\n","image = cv2.resize(image, (15, 15))\n","\n","# Display the grayscale image array\n","image"],"metadata":{"id":"iUCzdMAU9p6L","executionInfo":{"status":"ok","timestamp":1756319956490,"user_tz":-180,"elapsed":113,"user":{"displayName":"Mohamad Zbib","userId":"15661866840379818764"}},"colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"3e110fbe-f943-466d-ff5e-1f0706c9a999"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","        255, 255],\n","       [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","        255, 255],\n","       [255, 255, 255, 203, 153, 149, 255, 255, 255, 255, 255, 255, 255,\n","        255, 255],\n","       [255, 255, 242, 255,  36, 151, 149, 255, 255, 255, 255, 255, 255,\n","        255, 255],\n","       [255, 255, 149, 141, 149, 150, 155, 255, 255, 255, 255, 255, 255,\n","        255, 255],\n","       [255, 255, 151, 255, 255, 255, 251, 255, 255, 255, 255, 255, 255,\n","        255, 255],\n","       [255, 255, 151, 255, 255,  52, 179,  91, 170, 254, 242, 255, 255,\n","        255, 255],\n","       [255, 255, 151, 255, 174, 163, 145, 180, 145, 137, 255, 255, 255,\n","        255, 255],\n","       [255, 255, 148, 160, 157, 178, 149, 180, 145, 151, 174, 255, 255,\n","        255, 255],\n","       [255, 255, 255, 151, 151, 180, 151, 179, 151, 180, 148, 255, 255,\n","        255, 255],\n","       [255, 255, 255, 145, 151, 151, 151, 154, 151, 151, 151, 147, 150,\n","        255, 255],\n","       [255, 255, 255, 255, 130, 138, 151, 151, 151, 145, 151, 255, 255,\n","        255, 255],\n","       [255, 255, 255, 255,  38, 255, 165, 255, 236, 255, 177, 255, 255,\n","        255, 255],\n","       [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","        255, 255],\n","       [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","        255, 255]], dtype=uint8)"],"text/html":["<style>\n","      .ndarray_repr .ndarray_raw_data {\n","        display: none;\n","      }\n","      .ndarray_repr.show_array .ndarray_raw_data {\n","        display: block;\n","      }\n","      .ndarray_repr.show_array .ndarray_image_preview {\n","        display: none;\n","      }\n","      </style>\n","      <div id=\"id-c435db7f-cee9-4fd8-9a63-c6facdaf64a1\" class=\"ndarray_repr\"><pre>ndarray (15, 15) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAAPCAAAAAAevcqWAAAAkklEQVR4nIWPuwkCQQBE3+4ti8lpBWZiBxYjWICfQLQFQzNBjEWMDA4zixBLMNREEDRQjj3GwOXwIid7wzDMGFGRrSIOOJ7CsDSkh1qrpaIcbN4T+Z+8nUKo9nUOvZ2eJWfjc/c6TyMX63vDQT2LPHrl1so3LwBGsPCQFBCSAUbArJZC7vvfPVJbW920lyTz588HkwM6rRwmDOcAAAAASUVORK5CYII=\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","        255, 255],\n","       [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","        255, 255],\n","       [255, 255, 255, 203, 153, 149, 255, 255, 255, 255, 255, 255, 255,\n","        255, 255],\n","       [255, 255, 242, 255,  36, 151, 149, 255, 255, 255, 255, 255, 255,\n","        255, 255],\n","       [255, 255, 149, 141, 149, 150, 155, 255, 255, 255, 255, 255, 255,\n","        255, 255],\n","       [255, 255, 151, 255, 255, 255, 251, 255, 255, 255, 255, 255, 255,\n","        255, 255],\n","       [255, 255, 151, 255, 255,  52, 179,  91, 170, 254, 242, 255, 255,\n","        255, 255],\n","       [255, 255, 151, 255, 174, 163, 145, 180, 145, 137, 255, 255, 255,\n","        255, 255],\n","       [255, 255, 148, 160, 157, 178, 149, 180, 145, 151, 174, 255, 255,\n","        255, 255],\n","       [255, 255, 255, 151, 151, 180, 151, 179, 151, 180, 148, 255, 255,\n","        255, 255],\n","       [255, 255, 255, 145, 151, 151, 151, 154, 151, 151, 151, 147, 150,\n","        255, 255],\n","       [255, 255, 255, 255, 130, 138, 151, 151, 151, 145, 151, 255, 255,\n","        255, 255],\n","       [255, 255, 255, 255,  38, 255, 165, 255, 236, 255, 177, 255, 255,\n","        255, 255],\n","       [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","        255, 255],\n","       [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","        255, 255]], dtype=uint8)</pre></div><script>\n","      (() => {\n","      const titles = ['show data', 'hide data'];\n","      let index = 0\n","      document.querySelector('#id-c435db7f-cee9-4fd8-9a63-c6facdaf64a1 button').onclick = (e) => {\n","        document.querySelector('#id-c435db7f-cee9-4fd8-9a63-c6facdaf64a1').classList.toggle('show_array');\n","        index = (++index) % 2;\n","        document.querySelector('#id-c435db7f-cee9-4fd8-9a63-c6facdaf64a1 button').textContent = titles[index];\n","        e.preventDefault();\n","        e.stopPropagation();\n","      }\n","      })();\n","    </script>"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["**Explanation:**\n","\n","In this cell, we:\n","- Reload the original image (`/content/dino.png`) in **grayscale**.\n","- Resize the image to a **15√ó15** pixel dimension.\n","- Simply display the **NumPy array** of grayscale pixel values (ranging from 0 to 255).\n","\n","You can see the shape `(15, 15)` if you print `image.shape`. Every element corresponds to a single pixel's intensity.\n","\n","**Example**:\n","```python\n","print(\"Image Shape:\", image.shape)\n","print(\"A pixel example (row=0, col=0):\", image[0, 0])\n"],"metadata":{"id":"6HNFaR-GyDQT"}},{"cell_type":"markdown","source":["## **3Ô∏è‚É£ Load and Display Color Image**"],"metadata":{"id":"AupzHnwtiODw"}},{"cell_type":"code","source":["# Load and display the color image as a matrix\n","image = cv2.imread('/content/EECE490-690/Chapters/C2 - Preparing Data for Statistical Machine Learning Algorithms /dino.png')  # Load the image in color\n","image = cv2.resize(image, (15, 15))     # Resize to 15x15\n","\n","# Display the color image array\n","image"],"metadata":{"id":"n17q0FE1_g0Q","executionInfo":{"status":"ok","timestamp":1756319956491,"user_tz":-180,"elapsed":84,"user":{"displayName":"Mohamad Zbib","userId":"15661866840379818764"}},"colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"3b961c62-3d1f-4473-d5f4-eac14bcbb6ec"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [198, 211, 189],\n","        [ 73, 191, 108],\n","        [ 69, 187, 104],\n","        [239, 255, 249],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [246, 241, 243],\n","        [255, 255, 249],\n","        [ 22,  29,  56],\n","        [ 71, 189, 106],\n","        [ 69, 188, 103],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [ 71, 186, 106],\n","        [ 47, 181,  99],\n","        [ 66, 189, 103],\n","        [ 72, 189, 102],\n","        [128, 164, 147],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [ 71, 189, 106],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [251, 251, 251],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [ 71, 189, 106],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [  6,  43,  87],\n","        [ 34, 178, 236],\n","        [ 52, 104,  80],\n","        [ 25, 163, 240],\n","        [250, 255, 254],\n","        [238, 240, 248],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [ 71, 189, 106],\n","        [255, 255, 244],\n","        [ 66, 174, 216],\n","        [ 94, 196, 124],\n","        [ 65, 185,  96],\n","        [ 31, 182, 233],\n","        [ 69, 186,  93],\n","        [ 61, 171,  99],\n","        [248, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [ 65, 187, 103],\n","        [ 82, 199, 114],\n","        [ 91, 195, 108],\n","        [ 19, 184, 227],\n","        [ 64, 191,  99],\n","        [ 31, 182, 233],\n","        [ 71, 189,  88],\n","        [ 71, 189, 106],\n","        [ 29, 184, 210],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 249],\n","        [ 71, 189, 106],\n","        [ 71, 189, 106],\n","        [ 31, 182, 233],\n","        [ 69, 190, 106],\n","        [ 32, 180, 232],\n","        [ 71, 189, 106],\n","        [ 31, 182, 233],\n","        [ 61, 191,  98],\n","        [255, 253, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [ 87, 173, 113],\n","        [ 71, 189, 106],\n","        [ 71, 189, 106],\n","        [ 71, 189, 108],\n","        [ 80, 188, 115],\n","        [ 71, 189, 106],\n","        [ 71, 189, 106],\n","        [ 71, 189, 106],\n","        [ 65, 186, 101],\n","        [ 58, 191, 105],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [ 66, 157, 101],\n","        [ 88, 158, 118],\n","        [ 71, 189, 106],\n","        [ 71, 189, 106],\n","        [ 71, 189, 108],\n","        [ 67, 180, 106],\n","        [ 68, 191, 105],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [  6,  48,  30],\n","        [255, 254, 255],\n","        [144, 166, 171],\n","        [255, 255, 255],\n","        [236, 236, 236],\n","        [253, 255, 252],\n","        [111, 205, 148],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]]], dtype=uint8)"],"text/html":["<style>\n","      .ndarray_repr .ndarray_raw_data {\n","        display: none;\n","      }\n","      .ndarray_repr.show_array .ndarray_raw_data {\n","        display: block;\n","      }\n","      .ndarray_repr.show_array .ndarray_image_preview {\n","        display: none;\n","      }\n","      </style>\n","      <div id=\"id-9a039b14-bd6d-4fd1-9402-5a8e319c23dd\" class=\"ndarray_repr\"><pre>ndarray (15, 15, 3) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAAPCAIAAAC0tAIdAAABKklEQVR4nGP8//8/A9GAiXilKKqPX9nndSDXbU/mB4ZfuFQzQlzy/dMXTj42cTl7wwVG//7/3eU8A6tqFggVeLL8xe+fOgv0GRn+WL/QxWU2VDUDM8MF1zkQ5p8/fwi7GwLYdSLUt380zQyUWvLxN8P/9x9/4lL91Xn9zYiZqvLsJa/9ZuqI5zqvSeXiZ8ei+te/v8En6iRFp2z9kv6fjZmBgYGJ+TeXELvqzstYVO9zmfT12483v/7x/GNj/c/O+O8/A4PQjZ/1CuwTGP4hVDPC4zJiXdEHAYgrmTj/s39n/A619u/vn6wsRx2moahmYGBwnpcmxij4Xv4LRM+f7/8Yuf/vdpiGMPw/EmAzkPv/7/+EZav/////5s2bv///5J+dgqyAkYapCgCDU5UpQMFIegAAAABJRU5ErkJggg==\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[[255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [198, 211, 189],\n","        [ 73, 191, 108],\n","        [ 69, 187, 104],\n","        [239, 255, 249],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [246, 241, 243],\n","        [255, 255, 249],\n","        [ 22,  29,  56],\n","        [ 71, 189, 106],\n","        [ 69, 188, 103],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [ 71, 186, 106],\n","        [ 47, 181,  99],\n","        [ 66, 189, 103],\n","        [ 72, 189, 102],\n","        [128, 164, 147],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [ 71, 189, 106],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [251, 251, 251],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [ 71, 189, 106],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [  6,  43,  87],\n","        [ 34, 178, 236],\n","        [ 52, 104,  80],\n","        [ 25, 163, 240],\n","        [250, 255, 254],\n","        [238, 240, 248],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [ 71, 189, 106],\n","        [255, 255, 244],\n","        [ 66, 174, 216],\n","        [ 94, 196, 124],\n","        [ 65, 185,  96],\n","        [ 31, 182, 233],\n","        [ 69, 186,  93],\n","        [ 61, 171,  99],\n","        [248, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [ 65, 187, 103],\n","        [ 82, 199, 114],\n","        [ 91, 195, 108],\n","        [ 19, 184, 227],\n","        [ 64, 191,  99],\n","        [ 31, 182, 233],\n","        [ 71, 189,  88],\n","        [ 71, 189, 106],\n","        [ 29, 184, 210],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 249],\n","        [ 71, 189, 106],\n","        [ 71, 189, 106],\n","        [ 31, 182, 233],\n","        [ 69, 190, 106],\n","        [ 32, 180, 232],\n","        [ 71, 189, 106],\n","        [ 31, 182, 233],\n","        [ 61, 191,  98],\n","        [255, 253, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [ 87, 173, 113],\n","        [ 71, 189, 106],\n","        [ 71, 189, 106],\n","        [ 71, 189, 108],\n","        [ 80, 188, 115],\n","        [ 71, 189, 106],\n","        [ 71, 189, 106],\n","        [ 71, 189, 106],\n","        [ 65, 186, 101],\n","        [ 58, 191, 105],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [ 66, 157, 101],\n","        [ 88, 158, 118],\n","        [ 71, 189, 106],\n","        [ 71, 189, 106],\n","        [ 71, 189, 108],\n","        [ 67, 180, 106],\n","        [ 68, 191, 105],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [  6,  48,  30],\n","        [255, 254, 255],\n","        [144, 166, 171],\n","        [255, 255, 255],\n","        [236, 236, 236],\n","        [253, 255, 252],\n","        [111, 205, 148],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]]], dtype=uint8)</pre></div><script>\n","      (() => {\n","      const titles = ['show data', 'hide data'];\n","      let index = 0\n","      document.querySelector('#id-9a039b14-bd6d-4fd1-9402-5a8e319c23dd button').onclick = (e) => {\n","        document.querySelector('#id-9a039b14-bd6d-4fd1-9402-5a8e319c23dd').classList.toggle('show_array');\n","        index = (++index) % 2;\n","        document.querySelector('#id-9a039b14-bd6d-4fd1-9402-5a8e319c23dd button').textContent = titles[index];\n","        e.preventDefault();\n","        e.stopPropagation();\n","      }\n","      })();\n","    </script>"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["**Explanation:**\n","\n","1. **`cv2.imread('/content/images/data_for_labs/chapter_2/dino.png')`** defaults to loading images in **BGR** (Blue, Green, Red) format in OpenCV. Unlike the grayscale image, this now has 3 channels.  \n","2. **`cv2.resize(image, (15, 15))`** again resizes to 15√ó15.  \n","3. Printing the array displays a shape of **(15, 15, 3)**, indicating **height = 15**, **width = 15**, and **3 color channels**.\n","\n","For color images, each pixel is represented by three intensity values, one for each channel (B, G, R).\n"],"metadata":{"id":"bM_QrLOGyVas"}},{"cell_type":"markdown","source":["## **Key Takeaways**\n","\n","- **Images as Arrays**: A grayscale image can be viewed as a 2D array, while a color image generally has three channels (e.g., BGR or RGB).\n","- **Resizing**: Changing the height and width can be crucial for memory constraints and for standardizing input sizes in ML.\n","- **Thresholding**: Binary thresholding transforms each pixel into a 0 or 1, often used for segmentation or turning images into a simpler form.\n","- **OpenCV Default**: By default, OpenCV reads images in the BGR channel ordering, while other libraries (like Matplotlib) might expect RGB.\n"],"metadata":{"id":"zN-AFMrjye_k"}},{"cell_type":"markdown","source":["## **Reflection Questions**\n","\n","1. Why might we want to resize an image before passing it to a machine learning model?\n","2. What are some downstream computer vision tasks that benefit from binary thresholding?\n","3. How do the shapes of grayscale vs. color images differ numerically?\n"],"metadata":{"id":"82KFtmLhfzv6"}},{"cell_type":"markdown","source":["# **OPTIONAL: Basic Image Normalization**\n","\n","When working with images in Machine Learning, we often normalize or scale pixel values. This step can help many algorithms and neural networks train more effectively.\n","\n","For instance, **min-max normalization** maps pixel values from the range [0, 255] to [0, 1], making the image data more suitable for models that expect small input values or that are sensitive to large variations.\n","\n","**Why Normalize?**\n","- It ensures consistent data ranges (e.g., [0, 1]) across different images.\n","- Some machine learning models (like neural networks) converge faster when inputs are normalized.\n","- Normalization can reduce the impact of extreme pixel values (e.g., very bright or dark).\n","\n","Below is a quick demonstration of how to normalize a grayscale image to a [0, 1] range.\n"],"metadata":{"id":"hdYE6NVXmZzF"}},{"cell_type":"code","source":["# OPTIONAL CODE EXAMPLE: Basic Image Normalization\n","\n","import cv2\n","import numpy as np\n","\n","# 1) Load the image in grayscale\n","image = cv2.imread('/content/EECE490-690/Chapters/C2 - Preparing Data for Statistical Machine Learning Algorithms /dino.png', cv2.IMREAD_GRAYSCALE)\n","\n","# 2) Convert the image to float type for precise calculations\n","image_float = image.astype(np.float32)\n","\n","# 3) Compute min and max pixel values\n","min_val = image_float.min()\n","max_val = image_float.max()\n","\n","# 4) Perform min-max normalization to the range [0, 1]\n","normalized_image = (image_float - min_val) / (max_val - min_val)\n","\n","print(\"Before normalization:\")\n","print(\"  Data type:\", image.dtype)\n","print(\"  Pixel range:\", image.min(), \"to\", image.max())\n","\n","print(\"\\nAfter normalization:\")\n","print(\"  Data type:\", normalized_image.dtype)  # Should be float\n","print(\"  Pixel range:\", normalized_image.min(), \"to\", normalized_image.max())\n"],"metadata":{"id":"HpuQmSxfmkxp","executionInfo":{"status":"ok","timestamp":1756319956545,"user_tz":-180,"elapsed":54,"user":{"displayName":"Mohamad Zbib","userId":"15661866840379818764"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2b431d25-7324-4cd3-8182-92674a5b5dc2"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Before normalization:\n","  Data type: uint8\n","  Pixel range: 0 to 255\n","\n","After normalization:\n","  Data type: float32\n","  Pixel range: 0.0 to 1.0\n"]}]},{"cell_type":"markdown","source":["**Explanation:**\n","\n","1. We load the image in grayscale (cv2.IMREAD_GRAYSCALE).\n","2. We convert its data type to float32 for precision (originally it‚Äôs uint8).\n","3. We apply the standard formula for min-max normalization:\n","\n","$$\\text{normalized_pixel} = \\frac{\\text{pixel} - \\text{min_val}}{\\text{max_val} - \\text{min_val}}$$\n","\n","4. After normalization, the pixel range should be [0.0, 1.0].\n"],"metadata":{"id":"hFzppGwFpAGF"}},{"cell_type":"markdown","source":["###**TODO: Practice Normalizing an Image**\n","\n","Try one or more of the following mini-tasks to solidify your understanding:\n","\n","1. **Custom Normalization Range**  \n","   - Modify the code above so the image is scaled to the range `[0, 255]` again (effectively re-scaling after normalization).  \n","   - *Hint:* You might multiply the normalized image by 255 and convert it back to `uint8`.\n","\n","2. **Color Image Normalization**  \n","   - Load the **color** image (BGR) instead of grayscale.  \n","   - Convert it to a float type and normalize each channel to [0, 1].  \n","   - Print or observe the pixel range for each channel.\n","\n","3. **Compare Histograms** (Optional)  \n","   - Use `cv2.calcHist()` or any other method to compute the histogram of the original grayscale image versus the normalized one.  \n","   - See how normalization alters the distribution of pixel values.\n","   \n","Document your observations in a new Markdown cell. If you feel comfortable, show a quick visualization of the normalized image using `matplotlib` to confirm that the image still ‚Äúlooks‚Äù the same, just with different numerical values.\n"],"metadata":{"id":"A9DZvXalmly_"}},{"cell_type":"code","source":["#TODO HERE"],"metadata":{"id":"gJA2jpmFmztV","executionInfo":{"status":"ok","timestamp":1756319956547,"user_tz":-180,"elapsed":20,"user":{"displayName":"Mohamad Zbib","userId":"15661866840379818764"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# üîó **Additional Resources**\n","\n","- **OpenCV Documentation:** [https://docs.opencv.org/master/](https://docs.opencv.org/master/)\n","- **NumPy Documentation (array manipulations):** [https://numpy.org/doc/](https://numpy.org/doc/)\n","- **Stanford CS231n (Image Data):** [http://cs231n.github.io/python-numpy-tutorial/#images](http://cs231n.github.io/python-numpy-tutorial/#images)\n","- **OpenCV Basic Image Operations:** [https://docs.opencv.org/master/dc/d71/tutorial_py_emi.html](https://docs.opencv.org/master/dc/d71/tutorial_py_emi.html)\n","\n","\n","---\n","\n","# üìö **Further Suggested Readings**\n","\n","- **Pillow (PIL) Documentation:** [https://pillow.readthedocs.io/en/stable/](https://pillow.readthedocs.io/en/stable/)  \n","  Useful for opening and processing images in Python without OpenCV.\n","\n","- **scikit-image Documentation:** [https://scikit-image.org/docs/stable/](https://scikit-image.org/docs/stable/)  \n","  A Python library for advanced image processing tasks such as edge detection, filters, and morphological operations.\n","\n","- **Image Preprocessing Tips:**  \n","  Articles and tutorials on techniques like normalization, augmentation, and how these impact model performance:\n","  1. [How to Configure Image Data Augmentation When Training Deep Learning Neural Networks (Machine Learning Mastery)](https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/)  \n","  2. [Keras Image Preprocessing Layers (Official Keras Docs)](https://keras.io/api/preprocessing/image/)  \n","  3. [Data Augmentation | TensorFlow Core](https://www.tensorflow.org/tutorials/images/data_augmentation)  \n","  4. [CS231n: Normalization and BatchNorm](http://cs231n.github.io/neural-networks-2/#batchnorm)\n","\n","---\n","\n","# üîó **Kaggle Resources and Example Notebooks**\n","\n","- [**Image Processing Basics with OpenCV for Beginners**](https://www.kaggle.com/code/zeeshanlatif/image-processing-basics-with-opencv-for-beginners): A beginner-friendly notebook demonstrating fundamental OpenCV operations like reading images, color space conversions, and basic transformations.  \n","- [**Complete Guide to Image Processing with OpenCV**](https://www.kaggle.com/code/natigmamishov/complete-guide-to-image-processing-with-opencv): Covers advanced OpenCV techniques, including filtering, edge detection, and morphological transformations.  \n","- [**Learn OpenCV by Examples - with Python**](https://www.kaggle.com/code/bulentsiyah/learn-opencv-by-examples-with-python): Provides practical examples for tasks like image thresholding, contour detection, and histogram equalization using OpenCV and Python.  "],"metadata":{"id":"D8RRT5Y0gKW_"}}]}