{"cells":[{"cell_type":"markdown","id":"a0ac6a3e","metadata":{"id":"a0ac6a3e"},"source":["# **Supervised Machine Learning: Regression Lab**\n","This notebook demonstrates how to build and evaluate several regression modelsâ€”**Linear Regression**, **K-Neighbors Regressor**, and **Support Vector Regression**â€”using the California Housing dataset. We'll compare performance via MSE, MAE, and RÂ² metrics.\n","\n","---\n","\n","## ðŸ”—**Library Overview**\n","\n","- **NumPy & pandas**: For data manipulation and numerical calculations.\n","- **Matplotlib**: Optional for visualizing data or residual plots.\n","- **scikit-learn**:\n","  - `fetch_california_housing()`: Built-in dataset with California housing features.\n","  - `train_test_split()`: Splitting into train and test sets.\n","  - `StandardScaler()`: For scaling features (optional if your models benefit from normalization).\n","  - **Models**: `LinearRegression`, `KNeighborsRegressor`, `SVR`.\n","  - **Metrics**: MSE, MAE, RÂ² score for regression tasks.\n"]},{"cell_type":"markdown","id":"667c2791","metadata":{"id":"667c2791"},"source":["## **Step 1: Import Required Libraries**"]},{"cell_type":"code","execution_count":null,"id":"0e79cce1","metadata":{"id":"0e79cce1"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"]},{"cell_type":"markdown","source":["**Explanation**  \n","Here we import the essential libraries for:\n","- Data handling (`numpy`, `pandas`)\n","- Visualization (`matplotlib` - in case we want to plot)\n","- Machine learning utilities from **scikit-learn** (train/test split, metrics)\n","- Feature scaling (`StandardScaler`) if needed\n"],"metadata":{"id":"4kH48yyfJUBl"},"id":"4kH48yyfJUBl"},{"cell_type":"markdown","id":"623e2120","metadata":{"id":"623e2120"},"source":["## **Step 2: Load Dataset**\n"]},{"cell_type":"code","execution_count":null,"id":"16e0cdeb","metadata":{"id":"16e0cdeb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740039915339,"user_tz":-120,"elapsed":6564,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}},"outputId":"a0d22f62-a089-469c-e0ba-b71814f90527"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training samples: 16512, Testing samples: 4128\n"]}],"source":["from sklearn.datasets import fetch_california_housing\n","\n","# Load the dataset\n","data = fetch_california_housing()\n","\n","# Separate features (X) and target (y)\n","X, y = data.data, data.target\n","\n","# Create train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y,\n","                                                    test_size=0.2,\n","                                                    random_state=42)\n","\n","# Display the number of training/testing samples\n","print(f'Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}')\n"]},{"cell_type":"markdown","source":["**Explanation**  \n","We load the built-in **California Housing** dataset from scikit-learn, which contains data about California districts with features like:\n","- Median income\n","- Average number of rooms\n","- Latitude/longitude\n","- Etc.\n","\n","The target variable (`y`) is the median house value (in 100k USD).  \n","We then split the dataset into **training** (80%) and **test** (20%) subsets, printing the sample counts for each.\n"],"metadata":{"id":"qrZ3biQKJZyj"},"id":"qrZ3biQKJZyj"},{"cell_type":"markdown","id":"509ea43c","metadata":{"id":"509ea43c"},"source":["## **Step 3: Train and Evaluate Regression Models**"]},{"cell_type":"markdown","source":["### **Brief Overview of Each Regression Algorithm**\n","\n","In this lab, we compare three popular regression algorithms: **Linear Regression**, **K-Nearest Neighbors (KNN) Regression**, and **Support Vector Regression (SVR)**.\n","\n","#### **1. Linear Regression**\n","\n","- **Key Idea**: Linear Regression models the relationship between one or more independent variables (features) and a continuous target by fitting a **linear** function. Typically:\n","  \n","  $$\n","\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n\n","$$\n","\n","  \n","  The parameters (\\(\\beta_i\\)) are learned by **minimizing** the sum of squared residuals (Ordinary Least Squares).\n","  \n","- **Use Cases**: Predicting continuous values such as house prices, sales numbers, or any linear-ish trend.\n","- **Pros**: Simple, fast, easily interpretable coefficients (you can see how each feature affects the outcome).\n","- **Cons**: Assumes a linear relationship. Not well-suited if the data is significantly non-linear or has many complex interactions.\n","\n",">>>![Linear Regression Diagram](https://miro.medium.com/v2/resize%3Afit%3A1358/0%2AQp-81eR-lw3zr4Do.png)  \n","*Figure: Illustration of a Linear Regression model depicting the relationship between an independent variable and a dependent variable in Machine Learning.*\n","\n","\n","---\n","\n","#### **2. K-Nearest Neighbors (KNN) Regression**\n","\n","- **Key Idea**: Instead of learning an explicit function, KNN **stores** the training data and **predicts** the target for a new data point by looking at the **average (mean)** of the **k nearest neighbors** in feature space.\n","  \n","- **Use Cases**: Works reasonably well for smaller datasets with low to moderate dimensionality, where a non-parametric approach might capture local patterns better than a global linear model.\n","- **Pros**:  \n","  - Straightforward, intuitive.  \n","  - No real â€œtrainingâ€ phase, because data is just stored.\n","- **Cons**:  \n","  - Can be **computationally expensive** at prediction time (you have to find neighbors).  \n","  - Performance can degrade as the number of features grows (curse of dimensionality).  \n","  - Sensitive to outliers and scaling of features (itâ€™s often beneficial to scale or normalize your data).\n","\n",">>>![KNN Regression Diagram](https://media.geeksforgeeks.org/wp-content/uploads/20240617141430/download-%2819%29.png)  \n","*Figure: Illustration of K-Nearest Neighbors (KNN) regression, showing how predictions are made by averaging the k nearest neighbors in feature space.*\n","\n","\n","---\n","\n","#### **3. Support Vector Regression (SVR)**\n","\n","- **Key Idea**: SVR applies the principles of **Support Vector Machines** to regression tasks. Instead of trying to **classify** data points, it tries to fit them within a **tube** (often described by parameter \\(\\epsilon\\)) around a **best-fit line** (or hyperplane).  \n","  - Minimizes a loss function that ignores errors less than \\(\\epsilon\\), focusing on finding a flat hyperplane that fits most points within that margin.\n","- **Use Cases**: Good when the relationship between features and target is complex or high-dimensional. Can use **kernel tricks** (e.g., RBF, polynomial) to capture non-linearities.\n","- **Pros**:  \n","  - Can model **non-linear relationships** (with kernels).  \n","  - Has **regularization** built in, can be less prone to overfitting if tuned properly.\n","- **Cons**:  \n","  - Requires **hyperparameter tuning** (C, \\(\\epsilon\\), kernel parameters).  \n","  - Not as immediately interpretable as Linear Regression (especially with non-linear kernels).  \n","  - Can be slower on large datasets compared to simpler methods.\n","\n",">>>![Support Vector Regression Diagram](https://cdn-images-1.medium.com/max/1600/1%2Ars0EfF8RPVpgA-EfgAq85g.jpeg)  \n","*Figure: Illustration of Support Vector Regression (SVR) showing the epsilon-insensitive tube and support vectors.*\n","\n","\n","---\n","\n","### **Choosing the Right Algorithm**\n","\n","- **Linear Regression**: Start here when you suspect a roughly linear relationship and need interpretability.\n","- **KNN Regression**: Works well for smaller datasets or when relationships are highly non-linear but can be captured by local neighborhoods.\n","- **SVR**: Useful for more complex, high-dimensional data, or when you want a robust method that can handle non-linearity (via kernels), but be prepared for hyperparameter tuning.\n","\n"],"metadata":{"id":"QCyU8dhVOQmE"},"id":"QCyU8dhVOQmE"},{"cell_type":"markdown","id":"51445d49","metadata":{"id":"51445d49"},"source":["### **Linear Regression**"]},{"cell_type":"code","execution_count":null,"id":"d21a0f45","metadata":{"id":"d21a0f45","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740039917111,"user_tz":-120,"elapsed":134,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}},"outputId":"f5702cef-a4e7-4cc5-97e7-15e3c643b270"},"outputs":[{"output_type":"stream","name":"stdout","text":["MSE: 0.56\n","MAE: 0.53\n","R2 Score: 0.58\n"]}],"source":["from sklearn.linear_model import LinearRegression\n","\n","# Initialize Linear Regression\n","lr = LinearRegression()\n","\n","# Train the model\n","lr.fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred = lr.predict(X_test)\n","\n","# Evaluate the model\n","print(f'MSE: {mean_squared_error(y_test, y_pred):.2f}')\n","print(f'MAE: {mean_absolute_error(y_test, y_pred):.2f}')\n","print(f'R2 Score: {r2_score(y_test, y_pred):.2f}')\n"]},{"cell_type":"markdown","source":["**Explanation**  \n","1. We instantiate a **LinearRegression** model and fit it to `(X_train, y_train)`.  \n","2. We then predict on the test set `X_test`.  \n","3. We compute:\n","   - **MSE (Mean Squared Error)**: average of squared differences.  \n","   - **MAE (Mean Absolute Error)**: average of absolute differences.  \n","   - **RÂ² Score**: measures how much variance is explained by the model (1.0 = perfect).\n"],"metadata":{"id":"-rf187USJqyF"},"id":"-rf187USJqyF"},{"cell_type":"markdown","id":"30f6baf2","metadata":{"id":"30f6baf2"},"source":["### **K-Nearest Neighbors Regression**"]},{"cell_type":"code","execution_count":null,"id":"d91cd8b8","metadata":{"id":"d91cd8b8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740039942962,"user_tz":-120,"elapsed":129,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}},"outputId":"97dfb197-42a6-4673-9143-ec93c521dc14"},"outputs":[{"output_type":"stream","name":"stdout","text":["MSE: 1.12\n","MAE: 0.81\n","R2 Score: 0.15\n"]}],"source":["from sklearn.neighbors import KNeighborsRegressor\n","\n","# Initialize a KNN regressor with 5 neighbors\n","knn = KNeighborsRegressor(n_neighbors=5)\n","\n","# Train the model\n","knn.fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred = knn.predict(X_test)\n","\n","# Evaluate the model\n","print(f'MSE: {mean_squared_error(y_test, y_pred):.2f}')\n","print(f'MAE: {mean_absolute_error(y_test, y_pred):.2f}')\n","print(f'R2 Score: {r2_score(y_test, y_pred):.2f}')\n"]},{"cell_type":"markdown","source":["**Explanation**  \n","1. **KNeighborsRegressor** is a simple, instance-based learning technique that predicts a value by averaging the targets of its 'k' nearest neighbors in feature space.  \n","2. We set `n_neighbors=5` (the default) and fit the model.  \n","3. Similar to before, we predict on test data and compute MSE, MAE, and RÂ².\n"],"metadata":{"id":"p2xDbdyeJvm9"},"id":"p2xDbdyeJvm9"},{"cell_type":"markdown","id":"71e5ef97","metadata":{"id":"71e5ef97"},"source":["### **Support Vector Regression**"]},{"cell_type":"code","execution_count":null,"id":"c3b400b2","metadata":{"id":"c3b400b2"},"outputs":[],"source":["from sklearn.svm import SVR\n","\n","# Initialize an SVR with a linear kernel\n","svr = SVR(kernel='linear')\n","\n","# Train the model\n","svr.fit(X_train, y_train)\n","\n","# Predict\n","y_pred = svr.predict(X_test)\n","\n","# Evaluate performance\n","print(f'MSE: {mean_squared_error(y_test, y_pred):.2f}')\n","print(f'MAE: {mean_absolute_error(y_test, y_pred):.2f}')\n","print(f'R2 Score: {r2_score(y_test, y_pred):.2f}')\n"]},{"cell_type":"markdown","source":["**Explanation**  \n","1. We use **SVR** (Support Vector Regression) with a linear kernel.  \n","2. Fit the model on `(X_train, y_train)`.  \n","3. Predict on `X_test` and compute MSE, MAE, RÂ² to see how well the model explains the variance in the housing prices.\n"],"metadata":{"id":"1RhIckoAJx5T"},"id":"1RhIckoAJx5T"},{"cell_type":"markdown","source":["## ðŸ“š**OPTIONAL: Practice Tasks**\n","\n","1. **Experiment with Feature Scaling**  \n","   - In this notebook, we didnâ€™t demonstrate feature scaling in depth. Try applying a `StandardScaler` or `MinMaxScaler` on your features before training KNN or SVR.  \n","   - Observe how scaling impacts performance metrics (MSE, MAE, RÂ²).  \n","\n","2. **Hyperparameter Tuning**  \n","   - Explore hyperparameter tuning with `GridSearchCV` or `RandomizedSearchCV`:  \n","     - **KNN**: Number of neighbors (`n_neighbors`), distance metrics (Euclidean, Manhattan), weighting scheme (`uniform` vs. `distance`).  \n","     - **SVR**: Kernel type (`linear`, `rbf`, `poly`), regularization parameter `C`, epsilon `\\(\\epsilon\\)` in the epsilon-insensitive loss, or kernel parameters (e.g., `gamma` for RBF).  \n","   - Compare results to see which parameters yield the best performance.\n","\n","3. **Polynomial & Interaction Features**  \n","   - Apply `PolynomialFeatures` from `sklearn.preprocessing` to create polynomial and interaction terms (e.g., degree=2) and see if it improves Linear Regression results.  \n","   - Note if overfitting occurs and how to mitigate it (e.g., regularization).\n","\n","4. **Cross-Validation**  \n","   - Instead of a single train/test split, use **k-fold cross-validation** to get a more robust estimate of each modelâ€™s generalization ability.  \n","   - Check variance in cross-validation scores (e.g., using `cross_val_score`).\n","\n","5. **Compare with Other Regression Algorithms**  \n","   - Try additional models like **Random Forest Regressor**, **Gradient Boosting**, or **Ridge/Lasso** (regularized linear models).  \n","   - Observe how the performance compares, and consider trade-offs in terms of interpretability vs. accuracy.    \n","   \n","6. **Visualization Tasks**  \n","   - Create **residual plots** to see the distribution of errors for each model.  \n","   - Try partial dependence plots or feature importance (especially if you experiment with tree-based models) to understand which features matter most.\n","\n","By pursuing these tasks, youâ€™ll gain practical insights into **why** each model performs the way it does, **how** to systematically improve performance, and **when** a certain model or technique might be best for a real-world scenario.\n"],"metadata":{"id":"h2Ov1maRSVj5"},"id":"h2Ov1maRSVj5"},{"cell_type":"markdown","source":["##ðŸ”— **Further Reading**\n","1. **Scikit-Learn Documentation**:\n","   - [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n","   - [KNeighborsRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n","   - [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)\n","\n","2. **Hands-On Machine Learning with Scikit-Learn & TensorFlow**  \n","   - By AurÃ©lien GÃ©ron. Detailed coverage of regression, hyperparameter tuning, and more.\n","\n","3. **Kaggle Datasets**  \n","   - [House Prices - Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)\n","   - Try your regression models on real-world housing data.\n","\n","4. **California Housing** Additional Notebooks  \n","   - Check out the [scikit-learn User Guide](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset) for more examples.\n","\n","5.   **Introduction to Statistical Learning** (James, Witten, Hastie, Tibshirani) [Free PDF](https://www.statlearning.com/)  \n","     - Covers regression models, regularization, and more sophisticated techniques in a more theoretical but accessible manner.\n"],"metadata":{"id":"B6FvMqgmKGpc"},"id":"B6FvMqgmKGpc"}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}