{"cells":[{"cell_type":"markdown","id":"d6dc1dc5","metadata":{"id":"d6dc1dc5"},"source":["# **Supervised Machine Learning: Classification Lab**\n","In this lab, we will explore various classification algorithms using built-in datasets from scikit-learn.\n","\n","---\n","\n","## üìö**Classification on the Iris Dataset**\n","\n","Classification is a fundamental task in supervised machine learning, where the goal is to predict a **categorical** class label for a given input. In this notebook, we will explore multiple classification algorithms‚ÄîLogistic Regression, Naive Bayes, K-Nearest Neighbors, Support Vector Machine, and Decision Trees‚Äîusing the **Iris dataset**.\n","\n","We will:\n","- Load and understand the Iris dataset.\n","- Split the data into training and testing sets.\n","- Train different classifiers on the training set.\n","- Evaluate their performance on the test set using accuracy, confusion matrices, and classification reports.\n","\n","---\n","\n","##üîó **Library Overview**\n","\n","- **NumPy**: Numerical Python library for handling arrays and numerical operations.  \n","  [Official Docs](https://numpy.org/)\n","\n","- **pandas**: Data analysis and manipulation library with a convenient DataFrame structure.  \n","  [Official Docs](https://pandas.pydata.org/)\n","\n","- **Matplotlib**: A comprehensive library for creating static, animated, and interactive visualizations in Python.  \n","  [Official Docs](https://matplotlib.org/)\n","\n","- **Scikit-learn (`sklearn`)**:  \n","  - `model_selection` (train_test_split): for creating train/test sets.  \n","  - `datasets`: contains popular built-in datasets like Iris, Boston, etc.  \n","  - `preprocessing` (Optional: StandardScaler): for feature scaling.  \n","  - Classifiers (LogisticRegression, GaussianNB, KNeighborsClassifier, SVC, DecisionTreeClassifier).  \n","  - `metrics`: for evaluating classification performance (accuracy, confusion matrix, classification report).  \n","  [Official Docs](https://scikit-learn.org/stable/)\n","\n","---\n","\n","## **The Iris Dataset**\n","\n","The Iris dataset is a classic in machine learning:\n","- **150 samples** of iris flowers.\n","- **4 features**:\n","  1. Sepal length  \n","  2. Sepal width  \n","  3. Petal length  \n","  4. Petal width  \n","- **Target**: Iris species (setosa, versicolor, virginica).\n","\n"]},{"cell_type":"markdown","id":"0fdeaa8d","metadata":{"id":"0fdeaa8d"},"source":["## **Step 1: Import Required Libraries**"]},{"cell_type":"code","execution_count":1,"id":"f9e09203","metadata":{"id":"f9e09203","executionInfo":{"status":"ok","timestamp":1740470022377,"user_tz":-120,"elapsed":3771,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"]},{"cell_type":"markdown","source":["**Explanation**  \n","In this cell, we import the necessary libraries:\n","- `numpy` and `pandas` for data manipulation\n","- `matplotlib.pyplot` for plotting (if needed)\n","- `scikit-learn` functionalities (train_test_split, preprocessing, and evaluation metrics)\n"],"metadata":{"id":"DJY0XT8S-_YX"},"id":"DJY0XT8S-_YX"},{"cell_type":"markdown","id":"9f4ede0c","metadata":{"id":"9f4ede0c"},"source":["## **Step 2: Load Dataset**\n"]},{"cell_type":"code","execution_count":2,"id":"d463b5f2","metadata":{"id":"d463b5f2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740470023394,"user_tz":-120,"elapsed":1022,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}},"outputId":"14489ece-f79b-4581-d304-2ddc7e3b77aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training samples: 120, Testing samples: 30\n"]}],"source":["from sklearn.datasets import load_iris\n","\n","# Load Iris dataset\n","data = load_iris()\n","X, y = data.data, data.target\n","\n","# Split data into training (80%) and testing (20%) sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","print(f'Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}')"]},{"cell_type":"markdown","source":["**Explanation**  \n","Here we:\n","1. Load the classic **Iris dataset** from scikit-learn, which contains 150 samples of iris flowers with 4 features each (sepal length, sepal width, petal length, petal width) and a target variable (species).\n","2. Split the data into **training** and **testing** sets, with 80% for training and 20% for testing (random_state=42 for reproducibility).\n","3. Print the number of samples in each subset.\n"],"metadata":{"id":"Zq-e-yC-_Djj"},"id":"Zq-e-yC-_Djj"},{"cell_type":"markdown","id":"8e06f5a0","metadata":{"id":"8e06f5a0"},"source":["## **Step 3: Train and Evaluate Classification Models**\n","\n"]},{"cell_type":"markdown","source":["### **Brief Overview of Each Classification Algorithm**\n","\n","In this lab, we compare five popular classification algorithms: **Logistic Regression**, **Naive Bayes**, **K-Nearest Neighbors**, **Support Vector Machine**, and **Decision Trees**. Below is a short summary of each.\n","\n","#### **1. Logistic Regression**\n","- **Key Idea**: Extends linear regression to classification by applying a **sigmoid (logistic) function** to constrain outputs to a probability (0 to 1).  \n","- **Usage**: Often used for **binary classification** (spam vs. not spam), but can be generalized to multi-class (e.g., using one-vs-rest).  \n","- **Pros**: Easy to implement, interpretable coefficients, works well with linear separation.  \n","- **Cons**: Assumes a linear decision boundary, may underperform if data is not linearly separable.\n","\n","\n",">>>![Logistic Function](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)\n","\n",">>>*Figure: Logistic (sigmoid) curve mapping linear inputs to a 0-1 probability range.*\n","\n","\n","---\n","\n","### **2. Naive Bayes (GaussianNB)**\n","- **Key Idea**: Based on **Bayes‚Äô Theorem**, assumes independence among features given the class label.  \n","- **Usage**: Useful for text classification (spam detection, sentiment analysis) and other cases where independence assumptions roughly hold.  \n","- **Pros**: Fast to train, works well with high-dimensional data, can handle small datasets effectively.  \n","- **Cons**: Strong (often unrealistic) independence assumption, but still performs surprisingly well in many domains.\n","\n",">>>![Bayes Theorem Diagram](https://i.ytimg.com/vi/OByl4RJxnKA/maxresdefault.jpg)  \n","*Figure: Visual representation of Bayes' Theorem illustrating the relationship between prior, likelihood, and posterior probabilities.*\n","\n","\n","\n","---\n","\n","#### **3. K-Nearest Neighbors (KNN)**\n","- **Key Idea**: A **lazy learner** that uses the training data directly for classification by looking at the **‚Äòk‚Äô closest points** in the feature space.  \n","- **Usage**: Good for smaller datasets without too many features; commonly used in recommendation systems or simple classification tasks.  \n","- **Pros**: Simple concept, no explicit training phase.  \n","- **Cons**: Computationally expensive at prediction time (must search for nearest neighbors), sensitive to scaling of features and outliers.\n","\n",">![K-Nearest Neighbors Diagram](https://miro.medium.com/max/1151/0%2AItVKiyx2F3ZU8zV5)  \n","*Figure: Illustration of K-Nearest Neighbors algorithm classifying a new data point based on its nearest neighbors.*\n","\n","\n","---\n","\n","#### **4. Support Vector Machine (SVM)**\n","- **Key Idea**: Finds a **hyperplane** (or set of hyperplanes in higher-dimensional space) that best separates classes, maximizing the margin between them.  \n","- **Usage**: Very effective in high-dimensional spaces, can handle non-linear separations using **kernel tricks** (RBF, polynomial, etc.).  \n","- **Pros**: Powerful and flexible with kernels, often works well in practice even with limited data.  \n","- **Cons**: Can be tricky to tune (especially kernel parameters), not as interpretable as simpler linear models.\n","\n",">>>![Support Vector Machine Diagram](https://learnopencv.com/wp-content/uploads/2018/07/support-vectors-and-maximum-margin.png)  \n","*Figure: Illustration of a Support Vector Machine showing support vectors and the maximum margin hyperplane.*\n","\n","\n","\n","---\n","\n","#### **5. Decision Trees**\n","- **Key Idea**: Splits the feature space into regions by recursively asking ‚Äúyes/no‚Äù questions (e.g., `x_i <= threshold?`).  \n","- **Usage**: Widely used in many fields due to easy interpretability. Good for capturing non-linear relationships.  \n","- **Pros**: Highly interpretable, requires little data prep (no scaling needed), can handle mixed feature types (numeric/categorical).  \n","- **Cons**: Prone to **overfitting**, unstable splits if not tuned with pruning or ensemble methods (Random Forests, Gradient Boosting).\n","\n",">>> ![Decision Tree Diagram](https://eloquentarduino.github.io/wp-content/uploads/2020/08/DecisionTree.png)  \n","*Figure: Example of a Decision Tree illustrating decision nodes, branches, and classification outcomes.*\n","\n","\n","---\n","\n","### **When to Use Which Algorithm?**\n","- **Logistic Regression**: Baseline linear classifier, good interpretability and easy to implement.  \n","- **Naive Bayes**: Fast, works well for high-dimensional data (e.g., text classification).  \n","- **KNN**: Very intuitive, no explicit training. Works well for smaller datasets with few features.  \n","- **SVM**: Often robust in high-dimensional spaces, can adapt to non-linear boundaries with kernels.  \n","- **Decision Tree**: Easy to interpret, but can easily overfit if not regularized or combined into ensembles.\n","\n","Choose based on your data characteristics (size, dimensionality), interpretability requirements, and how much time you have for tuning.\n","\n"],"metadata":{"id":"n0GJxnNXGr5O"},"id":"n0GJxnNXGr5O"},{"cell_type":"markdown","id":"11629e46","metadata":{"id":"11629e46"},"source":["### **Logistic Regression**"]},{"cell_type":"code","execution_count":4,"id":"227bd270","metadata":{"id":"227bd270","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740470044940,"user_tz":-120,"elapsed":17,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}},"outputId":"13ffc413-4a7e-4e3b-f2f9-29c8d5462d8b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 1.00\n","[[10  0  0]\n"," [ 0  9  0]\n"," [ 0  0 11]]\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        10\n","           1       1.00      1.00      1.00         9\n","           2       1.00      1.00      1.00        11\n","\n","    accuracy                           1.00        30\n","   macro avg       1.00      1.00      1.00        30\n","weighted avg       1.00      1.00      1.00        30\n","\n"]}],"source":["from sklearn.linear_model import LogisticRegression\n","\n","# Create and train a Logistic Regression classifier\n","log_reg = LogisticRegression(max_iter=200)\n","log_reg.fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred = log_reg.predict(X_test)\n","\n","# Evaluate the model\n","print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n","print(confusion_matrix(y_test, y_pred))\n","print(classification_report(y_test, y_pred))\n"]},{"cell_type":"markdown","source":["**Explanation**  \n","- **Logistic Regression** is a simple yet popular linear model used for classification.  \n","- We instantiate a `LogisticRegression` model with `max_iter=200` (allowing it more iterations to converge).\n","- Train (fit) the model on `X_train, y_train`.\n","- Predict labels on the test set (`X_test`) and evaluate via:\n","  - **Accuracy**  \n","  - **Confusion Matrix**  \n","  - **Classification Report** (precision, recall, f1-score)\n"],"metadata":{"id":"zZ8uasRG_W8N"},"id":"zZ8uasRG_W8N"},{"cell_type":"markdown","id":"141edea5","metadata":{"id":"141edea5"},"source":["### **Naive Bayes**"]},{"cell_type":"code","execution_count":5,"id":"1af3607c","metadata":{"id":"1af3607c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740470046379,"user_tz":-120,"elapsed":12,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}},"outputId":"1095795e-0ea4-4df7-fb7a-a8ea4897904f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 1.00\n","[[10  0  0]\n"," [ 0  9  0]\n"," [ 0  0 11]]\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        10\n","           1       1.00      1.00      1.00         9\n","           2       1.00      1.00      1.00        11\n","\n","    accuracy                           1.00        30\n","   macro avg       1.00      1.00      1.00        30\n","weighted avg       1.00      1.00      1.00        30\n","\n"]}],"source":["from sklearn.naive_bayes import GaussianNB\n","\n","# Create and train a Gaussian Naive Bayes classifier\n","nb = GaussianNB()\n","nb.fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred = nb.predict(X_test)\n","\n","# Evaluate the model\n","print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n","print(confusion_matrix(y_test, y_pred))\n","print(classification_report(y_test, y_pred))\n"]},{"cell_type":"markdown","source":["**Explanation**  \n","- **Naive Bayes** (Gaussian) uses Bayes‚Äô theorem with the assumption of feature independence.\n","- We instantiate and train a `GaussianNB` classifier.\n","- Again, we predict on `X_test` and then print accuracy, confusion matrix, and classification report.\n"],"metadata":{"id":"x-hr5Nlq_duV"},"id":"x-hr5Nlq_duV"},{"cell_type":"markdown","id":"7fae1e41","metadata":{"id":"7fae1e41"},"source":["### **K-Nearest Neighbors (KNN)**"]},{"cell_type":"code","execution_count":null,"id":"ddb7d074","metadata":{"id":"ddb7d074","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740039516108,"user_tz":-120,"elapsed":161,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}},"outputId":"63517aaf-262c-4e2a-d052-b293d66106e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 1.00\n","[[10  0  0]\n"," [ 0  9  0]\n"," [ 0  0 11]]\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        10\n","           1       1.00      1.00      1.00         9\n","           2       1.00      1.00      1.00        11\n","\n","    accuracy                           1.00        30\n","   macro avg       1.00      1.00      1.00        30\n","weighted avg       1.00      1.00      1.00        30\n","\n"]}],"source":["from sklearn.neighbors import KNeighborsClassifier\n","\n","# Create and train a KNN classifier\n","knn = KNeighborsClassifier(n_neighbors=5)\n","knn.fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred = knn.predict(X_test)\n","\n","# Evaluate\n","print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n","print(confusion_matrix(y_test, y_pred))\n","print(classification_report(y_test, y_pred))\n"]},{"cell_type":"markdown","source":["**Explanation**  \n","- **K-Nearest Neighbors** is a simple, instance-based algorithm that classifies a data point based on how its neighbors are labeled.\n","- We create a `KNeighborsClassifier` with `n_neighbors=5` (the default is 5, but we make it explicit here).\n","- Train and predict, then evaluate as before.\n"],"metadata":{"id":"DTQTqOd4_kSo"},"id":"DTQTqOd4_kSo"},{"cell_type":"markdown","id":"cd054355","metadata":{"id":"cd054355"},"source":["### **Support Vector Machine (SVM)**"]},{"cell_type":"code","execution_count":null,"id":"483ee125","metadata":{"id":"483ee125","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739780988453,"user_tz":-120,"elapsed":6,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}},"outputId":"862441a6-3420-4c67-ad96-13f90a0180f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 1.00\n","[[10  0  0]\n"," [ 0  9  0]\n"," [ 0  0 11]]\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        10\n","           1       1.00      1.00      1.00         9\n","           2       1.00      1.00      1.00        11\n","\n","    accuracy                           1.00        30\n","   macro avg       1.00      1.00      1.00        30\n","weighted avg       1.00      1.00      1.00        30\n","\n"]}],"source":["from sklearn.svm import SVC\n","\n","# Create and train an SVM classifier with a linear kernel\n","svm = SVC(kernel='linear')\n","svm.fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred = svm.predict(X_test)\n","\n","# Evaluate\n","print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n","print(confusion_matrix(y_test, y_pred))\n","print(classification_report(y_test, y_pred))\n"]},{"cell_type":"markdown","source":["**Explanation**  \n","- **Support Vector Machine** with a linear kernel tries to find an optimal hyperplane that separates classes in a possibly high-dimensional space.\n","- We set `kernel='linear'` to keep it simpler. Train on `X_train` and evaluate as usual.\n"],"metadata":{"id":"Qp53MN7r_pVU"},"id":"Qp53MN7r_pVU"},{"cell_type":"markdown","id":"58900f89","metadata":{"id":"58900f89"},"source":["### **Decision Tree Classifier**"]},{"cell_type":"code","execution_count":null,"id":"955bfd06","metadata":{"id":"955bfd06","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739780988453,"user_tz":-120,"elapsed":5,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}},"outputId":"eb0dcced-74b9-4b9e-c629-54d4c92d4649"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 1.00\n","[[10  0  0]\n"," [ 0  9  0]\n"," [ 0  0 11]]\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        10\n","           1       1.00      1.00      1.00         9\n","           2       1.00      1.00      1.00        11\n","\n","    accuracy                           1.00        30\n","   macro avg       1.00      1.00      1.00        30\n","weighted avg       1.00      1.00      1.00        30\n","\n"]}],"source":["from sklearn.tree import DecisionTreeClassifier\n","\n","# Create and train a Decision Tree classifier\n","dt = DecisionTreeClassifier()\n","dt.fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred = dt.predict(X_test)\n","\n","# Evaluate\n","print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n","print(confusion_matrix(y_test, y_pred))\n","print(classification_report(y_test, y_pred))\n"]},{"cell_type":"markdown","source":["**Explanation**  \n","- **Decision Tree** builds a flowchart-like structure to decide class labels.\n","- We use `DecisionTreeClassifier` with default parameters (e.g., Gini impurity).\n","- As before, we fit the model and then see the performance metrics on test data.\n"],"metadata":{"id":"SgO-H1dv_vfD"},"id":"SgO-H1dv_vfD"},{"cell_type":"markdown","source":["##üìö **Additional Resources**\n","\n","Below are some resources and Kaggle notebooks you can explore to expand your understanding of classification techniques:\n","\n","\n","1. **Titanic - Machine Learning from Disaster (Kaggle Competition)**  \n","   - [Competition Page](https://www.kaggle.com/c/titanic)  \n","     A classic binary classification challenge (survival or not). Many public notebooks demonstrate advanced techniques like feature engineering, hyperparameter tuning, ensemble methods, etc.  \n","   - Example Notebooks:  \n","     - [A Data Science Framework: To Achieve 99% Accuracy (Beginner)](https://www.kaggle.com/code/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy)  \n","       Walks through the entire ML process, from data cleaning to model evaluation, focusing on classification.\n","\n","2. **Penguin Dataset**  \n","   - **Dataset**: [Palmer Penguins Dataset](https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data)  \n","   - **Sample Notebook**: [Penguin Classification with ML Models](https://www.kaggle.com/code/parulpandey/penguin-dataset-the-new-iris)  \n","   - Often touted as the ‚Äúnew Iris,‚Äù it‚Äôs a multi-class classification problem for three penguin species using numeric features like flipper length, body mass, etc.\n","\n","3. **Spam/Ham Email Classifier**  \n","   - **Dataset**: [SMS Spam Collection Dataset](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset)  \n","\n","4. **Scikit-Learn Official Documentation**  \n","   - [Classification User Guide](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)  \n","   - Detailed reference on implementing classification algorithms, handling imbalanced datasets, evaluating models, etc.\n","\n","5. **Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow (by Aur√©lien G√©ron)**  \n","   - Includes chapters on fundamental classification algorithms, hyperparameter tuning, and best practices.\n","\n","By exploring these resources, you‚Äôll see real-world data preprocessing, feature engineering, and advanced techniques that build upon the core classification methods demonstrated in this lab.\n"],"metadata":{"id":"BgBTDQGvBu42"},"id":"BgBTDQGvBu42"}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["0fdeaa8d","9f4ede0c"]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}